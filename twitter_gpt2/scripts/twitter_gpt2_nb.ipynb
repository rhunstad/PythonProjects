{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import time\n",
    "import re\n",
    "from random import randint\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "#Twitter API credentials\n",
    "consumer_key = 'NvmfyRI0y3lbskoDcFhNklWMX'\n",
    "consumer_secret = 'fClojvv7531V4kZ59M24tVvOBnva6bRhRxwloGu8uAJMc3NMsj'\n",
    "bearer_token = 'AAAAAAAAAAAAAAAAAAAAAPKfOAEAAAAAy2HePVkdpqJi743j30qdSV%2BQBqw%3DPw9lZl6qCrPQZVzeYLyADdxFh0KvfCGwe2iwt4VrRdoWdRqZ5o'\n",
    "access_key = \"\"\n",
    "access_secret = \"\"\n",
    "\n",
    "tweets_text = {}\n",
    "cleaned_tweets_text = {}\n",
    "generated_tweets = {}\n",
    "\n",
    "\n",
    "def save_file(out_dict, username, tweets):\n",
    "    json_tweets = json.dumps(out_dict)\n",
    "    now = re.sub(\" \", \"_\", str(datetime.now()))\n",
    "    now = re.sub(\"[:.]\", \"-\", now)\n",
    "    if tweets:\n",
    "        content = \"tweets\"\n",
    "    else:\n",
    "        content = \"gen\"\n",
    "    f = open(f'{username}_{content}__{now}.txt', 'w')\n",
    "    f.write(json_tweets)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_tweets(screen_name):\n",
    "    global tweets_text\n",
    "    global cleaned_tweets_text\n",
    "    #Twitter only allows access to a users most recent 3240 tweets with this method\n",
    "    \n",
    "    #authorize twitter, initialize tweepy\n",
    "    auth = tweepy.AppAuthHandler(consumer_key, consumer_secret)\n",
    "    api = tweepy.API(auth)\n",
    "    \n",
    "    #initialize a list to hold all the tweepy Tweets \n",
    "    alltweets = []\n",
    "    #make initial request for most recent tweets (200 is the maximum allowed count)\n",
    "    new_tweets = api.user_timeline(screen_name = screen_name,count=200, tweet_mode='extended')\n",
    "    \n",
    "    #save most recent tweets\n",
    "    alltweets.extend(new_tweets)\n",
    "    \n",
    "    #save the id of the oldest tweet less one\n",
    "    oldest = alltweets[-1].id - 1\n",
    "    \n",
    "    #keep grabbing tweets until there are no tweets left to grab\n",
    "    while len(new_tweets) > 0:\n",
    "        print(f\"getting tweets before {oldest}\")\n",
    "        \n",
    "        #all subsiquent requests use the max_id param to prevent duplicates\n",
    "        new_tweets = api.user_timeline(screen_name = screen_name,count=200,max_id=oldest, tweet_mode='extended')\n",
    "        \n",
    "        #save most recent tweets\n",
    "        alltweets.extend(new_tweets)\n",
    "        \n",
    "        #update the id of the oldest tweet less one\n",
    "        oldest = alltweets[-1].id - 1\n",
    "        \n",
    "        print(f\"...{len(alltweets)} tweets downloaded so far\")\n",
    "    \n",
    "    #transform the tweepy tweets into a dict that will populate the json text file: \n",
    "    # outtweets = {tweet.id_str: tweet._json for tweet in alltweets}\n",
    "    tweets_text = {tweet.id_str:tweet.full_text for tweet in alltweets}\n",
    "    \n",
    "    removelist = \".,'!?\\s\"\n",
    "\n",
    "    for tweet_id, tweet_text in tweets_text.items():\n",
    "        cleaned_tweet = re.sub(\"(@[A-Za-z0-9]+)|(_[A-Za-z0-9]+)|(\\w+:\\/\\/\\S+)\",\" \", tweet_text)\n",
    "        cleaned_tweet = re.sub(\"\\s+\", \" \", cleaned_tweet.strip())\n",
    "        cleaned_tweet = re.sub(\"RT\", \"\", cleaned_tweet)\n",
    "        # cleaned_tweet = re.sub(r'[^\\w'+removelist+']', '', cleaned_tweet)\n",
    "        cleaned_tweet = re.sub(\" : \", \"\", cleaned_tweet)\n",
    "        cleaned_tweets_text[tweet_id] = cleaned_tweet\n",
    "    \n",
    "    save_file(tweets_text, screen_name, True)\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate(t_id):\n",
    "    global generated_tweets\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "    t_list = list(cleaned_tweets_text.values())\n",
    "    seed = \" \".join([t_list[randint(0, len(t_list) - 1)] for i in range(3)]) + \". \"\n",
    "    seed_length = len(seed)\n",
    "    \n",
    "    print(\"Initialized...\")\n",
    "    \n",
    "    inputs = tokenizer.encode(seed, return_tensors='pt')\n",
    "    outputs = model.generate(inputs, max_length=200, do_sample=True)\n",
    "    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    tweet = text[seed_length:].strip()\n",
    "\n",
    "    generated_tweets[t_id] = tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in old tweets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized...\n",
      "Generated tweet 0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized...\n",
      "Generated tweet 1...\n",
      "I want to be reminded of some great old time radio host, Jimmy Stewart that i want to hear your music of choice.  Hear that song and listen it all night.  The only thing that doesn \n",
      "\n",
      "__________________________________ You know how your mom and dad make $50 a week, a lot of them. how many of those jobs are there that your mom is really into or that you could make a lot off of that money? how \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_new_tweets(pull_new, filename, username, num_gen):\n",
    "    global tweets_text\n",
    "    global cleaned_tweets_text\n",
    "    global generated_tweets\n",
    "    generated_tweets = {}\n",
    "    \n",
    "    if pull_new:\n",
    "        get_all_tweets(username)\n",
    "        print(\"Got new tweets...\")\n",
    "    else:\n",
    "        f = open(filename, \"r\")\n",
    "        tweets_text = json.loads(f.read())\n",
    "        \n",
    "        for tweet_id, tweet_text in tweets_text.items():\n",
    "            cleaned_tweet = re.sub(\"(@[A-Za-z0-9]+)|(_[A-Za-z0-9]+)|(\\w+:\\/\\/\\S+)\",\" \", tweet_text)\n",
    "            cleaned_tweet = re.sub(\"\\s+\", \" \", cleaned_tweet.strip())\n",
    "            cleaned_tweet = re.sub(\"RT\", \"\", cleaned_tweet)\n",
    "            # cleaned_tweet = re.sub(r'[^\\w'+removelist+']', '', cleaned_tweet)\n",
    "            cleaned_tweet = re.sub(\" : \", \"\", cleaned_tweet)\n",
    "            cleaned_tweets_text[tweet_id] = cleaned_tweet\n",
    "        print(\"Read in old tweets\")\n",
    "            \n",
    "    for i in range(num_gen):\n",
    "        generate(i)\n",
    "        print(f\"Generated tweet {i}...\")\n",
    "    \n",
    "    save_file(generated_tweets, username, False)\n",
    "    \n",
    "    for tweet in generated_tweets.values():\n",
    "        print(tweet, \"\\n\")\n",
    "            \n",
    "\n",
    "            \n",
    "get_new_tweets(False, 'rylandhunstad_tweets__2021-04-01_01-34-06-037050.txt', 'rylandhunstad', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
